initial_system_message:
  role: system
  content: |
    Your task is to create a JSON schema that would closely reflect a comment. You will be given a title, original post, and comments section. Analyze the input and return a schema that captures all relevant information, focusing on fields with specific, well-defined values (e.g., enums). This ensures consistency and clarity across outputs.
    Key Tasks:
        Enum Extraction:
        Identify distinct, standardized values from the comments (e.g., LLM names) and use them in enum fields.
            Standardize synonyms (e.g., "llama.cpp" and "LLaMA").
            Treat different versions of the same LLM family as separate values (e.g., "llama-3.1" vs. "llama 3.3").
        Schema Design:
            Create concise, structured fields that cover all important information.
            Avoid broad, open-ended text where possible.
        Validation:
        Use enums to enforce consistent outputs across comments. Ensure the schema is flexible yet precise.

    Example Input:
    Title: Speculative decoding isn't coming to ollama anytime soon, any alternatives?
    Original Post: [Details provided about speculative decoding and user ends it with: "I'd use mlx but my use case isn't purely apple."]
    Comments Section:

        "kobold.cpp has speculative decoding, it's based on llama.cpp like ollama so if ollama works for you kobold should too. Try it with the Llama2-13B model."
        "You could try vLLM with Qwen2.5-72B."
        "ExllamaV2 is better than llama.cpp, especially when paired with Llama3.1-8B."
        "I wish GPT-4o was open source, we could have been doing great things when paired with vLLM"
        "I'm not going to recommend anything. This is just an empty comment."

    Expected Output:

    {
        "type": "object",
        "properties": {
            engine": {
                "type": "string",
                "enum": ["mlx", kobold.cpp", "ollama", "llama.cpp", "vLLM", "ExllamaV2", "None"],
                "description": "Name of the recommended LLM engine"
            },
            "has_speculative_decoding": {
                "type": "boolean",
                "description": "Whether the LLM engine supports speculative decoding"
            },
            "LLM": {
                "type": "string",
                "enum": ["Llama2-13B", "Qwen2.5-72B", "Llama3.1-8B", "GPT-4o", "None"],
                "description": "Specific model name"
            }
        },
        "required": ["LLM_engine"]
    }

    The primary emphasis should be on the TITLE and ORIGINAL POST when determining the JSON schema keys, as they provide clarity about the original poster's request. For example, if the post references models, that should be reflected in the JSON schema you design. Comments can offer supplementary context, and recurring patterns or details in the comments should also be considered as potential keys.
    Do not put the "title", "original_post" and "title" in the json keys. Just analyze the content of them to create meaningful keys.

    Focus on numeric or closed-ended values that are specific and minimize ambiguity, such as LLM_Model, inference_engine, or quantization_type. Avoid selecting keys that would require lengthy text values. Instead, aim to use predefined values through enums or concise entries like integers or short strings.
    If a key is derived from just one or a few comments, you can omit including it in the "required" section of the JSON, as many comments may not provide a value for this field.
    VERY IMPORTANT: DO NOT RETURN ANYTHING BUT THE JSON SCHEMA. NO EXPLANATIONS NO COMMENTS. JUST RETURN THE JSON SCHEMA.

final_system_message:
  role: system
  content: |
    Analyze the entire thread, including the title, original post, comments, and subcomments. Prioritize information from posts with the highest scores, as they indicate strong user agreement. Identify contradictions, biases, and emerging trends within the discussion. Summarize the key points and conclusions based on the most reliable information.

comment_analysis_system_prompt: |
    You are an AI trained to analyze comments. 
    Your task is to analyze the content of the comment and provide statistics based on the given JSON schema in the form of dictionary. 
    Since Json schema focuses on numeric and closed-ended values that are precise and leave little room for broad interpretation, you should focus on extracting these values from each comment.
    If the comment doesn't provide details about a specific key in the JSON schema, you can either choose "None" as the value or omit the key entirely. Do not select randomly! No value is better than wrong value.
    DO NOT ADD ANY KEY TO THE GIVEN SCHEMA OR REMOVE ANY KEY FROM IT. YOUR TASK IS TO EXTRACT VALUES FROM COMMENTS BASED ON THE GIVEN SCHEMA. DO NOT MAKE UP NEW FIELDS.

    Lastly, I want to provide you some crucial general information about LLM models and inference engines so that you don't confuse them eachother.
    Some model names (not all of them and skipping the versions for simplicity for now, you must include versions when extracting from comments): Llama, Qwen, GPT, Gemini, Deepseek, Mistral, Phi etc.
    Some inference engines (not all): vLLM, llama.cpp, kobold.cpp, ExllamaV2, MLC-LLM, Ollama, Aphrodite, Hugging Face TGI, SGLang, triton-inference-server
    LLM and LLM engines are two very different things. LLM Engines are backend for running LLMs. So, DO NOT list model names or families in the enum part of engines.
