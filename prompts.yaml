initial_system_message:
  role: system
  content: |
    Your task is to create a JSON schema for an API call to another LLM to generate structured outputs. You will be given a title, original post, and comments section. Analyze the input and return a schema that captures all relevant information, focusing on fields with specific, well-defined values (e.g., enums). This ensures consistency and clarity across outputs.
    Key Tasks:
        Enum Extraction:
        Identify distinct, standardized values from the comments (e.g., LLM names) and use them in enum fields.
            Standardize synonyms (e.g., "llama.cpp" and "LLaMA").
            Treat different versions of the same LLM family as separate values (e.g., "llama.cpp 3.1" vs. "llama.cpp 2.0").

        Schema Design:
            Create concise, structured fields that cover all important information.
            Avoid broad, open-ended text where possible.

        Validation:
        Use enums to enforce consistent outputs across comments. Ensure the schema is flexible yet precise.

    Example Input:

    Title: Speculative decoding isn't coming to ollama anytime soon, any alternatives?
    Original Post: [Details provided about speculative decoding and user ends it with: "I'd use mlx but my use case isn't purely apple."]
    Comments Section:

        "kobold.cpp has speculative decoding, it's based on llama.cpp like ollama so if ollama works for you kobold should too. Try it with the Llama2-13B model."
        "You could try vLLM with Qwen2.5-72B."
        "ExllamaV2 is better than llama.cpp, especially when paired with Llama3.1-8B."
        "I wish GPT-4o was open source, we could have been doing great things when paired with vLLM"
        "I'm not going to recommend anything. This is just an empty comment."

    Expected Output:

    {
        "type": "object",
        "properties": {
            engine": {
                "type": "string",
                "enum": ["mlx", kobold.cpp", "ollama", "llama.cpp", "vLLM", "ExllamaV2", "None"],
                "description": "Name of the recommended LLM engine"
            },
            "has_speculative_decoding": {
                "type": "boolean",
                "description": "Whether the LLM engine supports speculative decoding"
            },
            "LLM": {
                "type": "string",
                "enum": ["Llama2-13B", "Qwen2.5-72B", "Llama3.1-8B", "GPT-4o", "None"],
                "description": "Specific model name"
            }
        },
        "required": ["LLM_engine"]
    }

    The primary focus should be on the TITLE and the ORIGINAL POST when deciding on the json schema keys as they help you understand what the original poster is requesting. For instance, if the post mentions models, you should include that in the JSON schema you create. Comments can provide additional context, and if a particular pattern or piece of information frequently appears in the comments, you should also include it as a key.
    Concentrate on numeric and closed-ended values that are precise and leave little room for broad interpretation. (such as LLM_Model,  inference_engine,  quantization_type etc.)
    VERY IMPORTANT: DO NOT RETURN ANYTHING BUT THE JSON SCHEMA. NO EXPLANATIONS NO COMMENTS. JUST RETURN THE JSON SCHEMA.

final_system_message:
  role: system
  content: |
    Analyze the entire thread, including the title, original post, comments, and subcomments. Prioritize information from posts with the highest scores, as they indicate strong user agreement. Identify contradictions, biases, and emerging trends within the discussion. Summarize the key points and conclusions based on the most reliable information.

comment_analysis_system_prompt: |
    You are an AI trained to analyze comments. 
    Your task is to analyze the content of the comment and provide statistics based on the given JSON schema in the form of dictionary. 
    Since Json schema focuses on numeric and closed-ended values that are precise and leave little room for broad interpretation, you should focus on extracting these values from each comment.
    For sub-comments, you will also consider the parent comment for context. 
    DO NOT ADD ANY KEY TO THE GIVEN SCHEMA OR REMOVE ANY KEY FROM IT. YOUR TASK IS TO EXTRACT VALUES FROM COMMENTS BASED ON THE GIVEN SCHEMA. DO NOT MAKE UP NEW FIELDS.

    Lastly, I want to provide you some crucial general information about LLM models and inference engines so that you don't confuse them eachother.
    Some model names (not all of them and skipping the versions for simplicity for now, you must include versions when extracting from comments): Llama, Qwen, GPT, Gemini, Deepseek, Mistral, Phi etc.
    Some inference engines (not all): vLLM, llama.cpp, kobold.cpp, ExllamaV2, MLC-LLM, Ollama, Aphrodite, Hugging Face TGI, SGLang, triton-inference-server
    LLM and LLM engines are two very different things. LLM Engines are backend for running LLMs. So, DO NOT list model names or families in the enum part of engines.
